{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bdb210f-ba47-476c-815b-b362dbbd228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3608d3a7-cd34-402a-9a17-634105ec8c9c",
   "metadata": {},
   "source": [
    "#### Declare and Assign Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb3fdd4f-279a-494f-a88e-08430b1025cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"adventureworks\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"root\",\n",
    "        \"password\" : \"mypassword\",\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"cluster_location\" : \"atlas\",\n",
    "    \"user_name\" : \"miamccarrick\",\n",
    "    \"password\" : \"mypassword\",\n",
    "    \"cluster_name\" : \"mycluster\",\n",
    "    \"cluster_subnet\" : \"zl4ms\",\n",
    "    \"db_name\" : \"northwind\",\n",
    "    \"collection\" : \"\",\n",
    "    \"null_column_threshold\" : 0.5\n",
    "}\n",
    "\n",
    "\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'adventureworks')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "sales_stream_dir = os.path.join(stream_dir, 'sales')\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"northwind_data_lakehouse\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "sales_output_bronze = os.path.join(database_dir, 'fact_sales', 'bronze')\n",
    "sales_output_silver = os.path.join(database_dir, 'fact_sales', 'silver')\n",
    "sales_output_gold = os.path.join(database_dir, 'fact_sales', 'gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84b5cc-6c16-4c76-91d0-926c18e8e9c7",
   "metadata": {},
   "source": [
    "### Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "844575d2-55ad-49e3-b182-b194b6509a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    \n",
    "    return df_dropped\n",
    "    \n",
    "    \n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Northwind Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "    \n",
    "    \n",
    "# TODO: Rewrite this to leverage PySpark?\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    \n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34052074-d68f-4336-9c9d-834b1f69bd90",
   "metadata": {},
   "source": [
    "### Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e08bfa9d-a41e-4c08-93c0-96af14f053e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory '/Users/miamccarrick/DS2002/Project 2/spark-warehouse/northwind_data_lakehouse.db' has been removed successfully.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31186ec8-84a4-493f-9a74-6dfc89bc8f19",
   "metadata": {},
   "source": [
    "### Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6bb536-d317-43ee-a480-6f339e31f5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/25 11:22:47 WARN Utils: Your hostname, Mias-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 172.25.64.204 instead (on interface en0)\n",
      "25/04/25 11:22:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/miamccarrick/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/miamccarrick/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-90ae5798-958f-4738-9000-d6b6a51d852f;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      ":: resolution report :: resolve 143ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-90ae5798-958f-4738-9000-d6b6a51d852f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n",
      "25/04/25 11:22:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.25.64.204:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Northwind Data Lakehouse (Medallion Architecture)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x151012e40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.3.0\", \"mysql-connector-j-9.3.0.jar\")\n",
    "#mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5795390d-5a51-4947-9a7c-1e7270713d7b",
   "metadata": {},
   "source": [
    "### Create a New Metadata Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5c5571-56d5-4e81-8466-04ab97fd3ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Lab 06 Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Lab 6.0');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1ff4b-ab06-4976-89eb-14d2d025fc58",
   "metadata": {},
   "source": [
    "#### Getting Data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89381575-fa95-43ed-8803-6e7031624cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sql_products = \"\"\"\n",
    "SELECT * FROM dim_products\n",
    "\"\"\"\n",
    "product_data = get_mysql_dataframe(spark, sql_products, **mysql_args)\n",
    "pandas_df = product_data.toPandas()\n",
    "pandas_df.to_json(\"products.json\", orient=\"records\", lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa102200-dcd2-4b7a-b2c0-a1c1bf6e722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Populate MongoDB with product data\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "data_dir = os.getcwd()\n",
    "\n",
    "json_files = {\"products\" : 'products.json'}\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], data_dir, json_files)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22ef837e-b756-42e8-a207-0cdcca242e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Extract Products from MongoDB\n",
    "mongodb_args[\"collection\"] = \"products\"\n",
    "\n",
    "df_dim_products = get_mongodb_dataframe(spark, **mongodb_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f38553-2c09-4e04-b907-1096f4c902b5",
   "metadata": {},
   "source": [
    "#### Make Necessary Transformations to the New Dataframe¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e9daf4-eb08-446f-912d-754975829a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>product_id</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>ListPrice</th>\n",
       "      <th>Color</th>\n",
       "      <th>ProductLine</th>\n",
       "      <th>ProductModelID</th>\n",
       "      <th>ProductSubcategoryID</th>\n",
       "      <th>ReorderPoint</th>\n",
       "      <th>SafetyStockLevel</th>\n",
       "      <th>SellStartDate</th>\n",
       "      <th>StandardCost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>750</td>\n",
       "      <td>1000</td>\n",
       "      <td>896659200000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>750</td>\n",
       "      <td>1000</td>\n",
       "      <td>896659200000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  product_id             Name ProductNumber  ListPrice Color  \\\n",
       "0            1           1  Adjustable Race       AR-5381        0.0  None   \n",
       "1            2           2     Bearing Ball       BA-8327        0.0  None   \n",
       "\n",
       "  ProductLine  ProductModelID  ProductSubcategoryID  ReorderPoint  \\\n",
       "0        None             NaN                   NaN           750   \n",
       "1        None             NaN                   NaN           750   \n",
       "\n",
       "   SafetyStockLevel  SellStartDate  StandardCost  \n",
       "0              1000   896659200000           0.0  \n",
       "1              1000   896659200000           0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'customer_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_products = df_dim_products.withColumnRenamed(\"ProductID\", \"product_id\")\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['product_key', 'product_id', 'Name', 'ProductNumber','ListPrice', 'Color', 'ProductLine', \n",
    "                   'ProductModelID', 'ProductSubcategoryID', 'ReorderPoint', 'SafetyStockLevel', 'SellStartDate', 'StandardCost']\n",
    "\n",
    "df_dim_products = df_dim_products[ordered_columns]\n",
    "df_dim_products.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e1ac1-1843-4d4e-91c4-a6e0cbc1ef44",
   "metadata": {},
   "source": [
    "#### Save df_dim_products to Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33db395d-e931-4c96-8efd-400329e4725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dim_products.write.saveAsTable(f\"{dest_database}.dim_products\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2999f-0e90-4859-865e-bc8d5692f644",
   "metadata": {},
   "source": [
    "#### Getting Data from MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "953d838a-9d10-4068-b411-2610eff91995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Customers data\n",
    "sql_customers = \"SELECT * FROM adventureworks.dim_customers\"\n",
    "df_customers = get_mysql_dataframe(spark, sql_customers, **mysql_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45424957-4200-41ea-9f3d-e019fba93f3a",
   "metadata": {},
   "source": [
    "#### Save as the dim_customers table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356da097-5b7c-46a6-9cca-a964e9b42398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>CustomerType</th>\n",
       "      <th>TerritoryID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AW00000001</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>AW00000002</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id AccountNumber CustomerType  TerritoryID\n",
       "0             1            1    AW00000001            S            1\n",
       "1             2            2    AW00000002            S            1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dim_customers = df_customers.withColumnRenamed(\"CustomerID\", \"customer_id\")\n",
    "\n",
    "ordered_columns = ['customer_key', 'customer_id', 'AccountNumber',\n",
    "                  'CustomerType', 'TerritoryID']  \n",
    "\n",
    "df_dim_customers = df_dim_customers.select(ordered_columns)\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3963318f-29b4-49a8-a0ad-c4fcfe2880ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ae74d4-10b6-41b0-9e01-265e32d1ec1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|        customer_key|              bigint|   NULL|\n",
      "|         customer_id|              bigint|   NULL|\n",
      "|       AccountNumber|              string|   NULL|\n",
      "|        CustomerType|              string|   NULL|\n",
      "|         TerritoryID|              bigint|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|northwind_data_la...|       |\n",
      "|               Table|       dim_customers|       |\n",
      "|        Created Time|Fri Apr 25 11:23:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.5|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/miamc...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>CustomerType</th>\n",
       "      <th>TerritoryID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AW00000001</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>AW00000002</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id AccountNumber CustomerType  TerritoryID\n",
       "0             1            1    AW00000001            S            1\n",
       "1             2            2    AW00000002            S            1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_customers;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_customers LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e5268f-7e7c-4484-8548-ebb648d8c8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date_name</th>\n",
       "      <th>date_name_us</th>\n",
       "      <th>date_name_eu</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>is_last_day_of_month</th>\n",
       "      <th>calendar_quarter</th>\n",
       "      <th>calendar_year</th>\n",
       "      <th>calendar_year_month</th>\n",
       "      <th>calendar_year_qtr</th>\n",
       "      <th>fiscal_month_of_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_year_month</th>\n",
       "      <th>fiscal_year_qtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000/01/01</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>2000/01/02</td>\n",
       "      <td>01/02/2000</td>\n",
       "      <td>02/01/2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date    date_name date_name_us date_name_eu  day_of_week  \\\n",
       "0  20000101  2000-01-01  2000/01/01   01/01/2000   01/01/2000             7   \n",
       "1  20000102  2000-01-02  2000/01/02   01/02/2000   02/01/2000             1   \n",
       "\n",
       "  day_name_of_week  day_of_month  day_of_year weekday_weekend  ...  \\\n",
       "0       Saturday               1            1      Weekend     ...   \n",
       "1       Sunday                 2            2      Weekend     ...   \n",
       "\n",
       "   is_last_day_of_month calendar_quarter  calendar_year calendar_year_month  \\\n",
       "0                     N                1           2000          2000-01      \n",
       "1                     N                1           2000          2000-01      \n",
       "\n",
       "   calendar_year_qtr  fiscal_month_of_year fiscal_quarter fiscal_year  \\\n",
       "0         2000Q1                         7              3        2000   \n",
       "1         2000Q1                         7              3        2000   \n",
       "\n",
       "   fiscal_year_month  fiscal_year_qtr  \n",
       "0         2000-07          2000Q3      \n",
       "1         2000-07          2000Q3      \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Date Data\n",
    "sql_dim_date = \"SELECT * FROM adventureworks.dim_date\"\n",
    "df_dim_date = get_mysql_dataframe(spark, sql_dim_date, **mysql_args)\n",
    "df_dim_date.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51db0e9-e5c5-4689-b39a-6f4ef4756455",
   "metadata": {},
   "source": [
    "#### Save as the dim_date table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a05ee9c0-2ade-42e4-a5e7-b937fc44af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b06865e-c45e-4d51-86d7-60097a6db95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|            date_key|      int|   NULL|\n",
      "|           full_date|     date|   NULL|\n",
      "|           date_name| char(11)|   NULL|\n",
      "|        date_name_us| char(11)|   NULL|\n",
      "|        date_name_eu| char(11)|   NULL|\n",
      "|         day_of_week|  tinyint|   NULL|\n",
      "|    day_name_of_week| char(10)|   NULL|\n",
      "|        day_of_month|  tinyint|   NULL|\n",
      "|         day_of_year|      int|   NULL|\n",
      "|     weekday_weekend| char(10)|   NULL|\n",
      "|        week_of_year|  tinyint|   NULL|\n",
      "|          month_name| char(10)|   NULL|\n",
      "|       month_of_year|  tinyint|   NULL|\n",
      "|is_last_day_of_month|  char(1)|   NULL|\n",
      "|    calendar_quarter|  tinyint|   NULL|\n",
      "|       calendar_year|      int|   NULL|\n",
      "| calendar_year_month| char(10)|   NULL|\n",
      "|   calendar_year_qtr| char(10)|   NULL|\n",
      "|fiscal_month_of_year|  tinyint|   NULL|\n",
      "|      fiscal_quarter|  tinyint|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date_name</th>\n",
       "      <th>date_name_us</th>\n",
       "      <th>date_name_eu</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>is_last_day_of_month</th>\n",
       "      <th>calendar_quarter</th>\n",
       "      <th>calendar_year</th>\n",
       "      <th>calendar_year_month</th>\n",
       "      <th>calendar_year_qtr</th>\n",
       "      <th>fiscal_month_of_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_year_month</th>\n",
       "      <th>fiscal_year_qtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000/01/01</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>2000/01/02</td>\n",
       "      <td>01/02/2000</td>\n",
       "      <td>02/01/2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date    date_name date_name_us date_name_eu  day_of_week  \\\n",
       "0  20000101  2000-01-01  2000/01/01   01/01/2000   01/01/2000             7   \n",
       "1  20000102  2000-01-02  2000/01/02   01/02/2000   02/01/2000             1   \n",
       "\n",
       "  day_name_of_week  day_of_month  day_of_year weekday_weekend  ...  \\\n",
       "0       Saturday               1            1      Weekend     ...   \n",
       "1       Sunday                 2            2      Weekend     ...   \n",
       "\n",
       "   is_last_day_of_month calendar_quarter  calendar_year calendar_year_month  \\\n",
       "0                     N                1           2000          2000-01      \n",
       "1                     N                1           2000          2000-01      \n",
       "\n",
       "   calendar_year_qtr  fiscal_month_of_year fiscal_quarter fiscal_year  \\\n",
       "0         2000Q1                         7              3        2000   \n",
       "1         2000Q1                         7              3        2000   \n",
       "\n",
       "   fiscal_year_month  fiscal_year_qtr  \n",
       "0         2000-07          2000Q3      \n",
       "1         2000-07          2000Q3      \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d474ba85-98e7-4ab8-bced-e586bd535514",
   "metadata": {},
   "source": [
    "#### Getting Data from local CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd41b5cc-0f8a-4427-bd4e-d4131cf219ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query products to export as CSV\n",
    "sql_salesperson = \"\"\"\n",
    "SELECT SalesPersonID, TerritoryID, SalesQuota, Bonus,\n",
    "CommissionPct, SalesYTD, SalesLastYear\n",
    "FROM adventureworks.salesperson\n",
    "\"\"\"\n",
    "salesperson_data = get_mysql_dataframe(spark, sql_salesperson, **mysql_args)\n",
    "salesperson_data = salesperson_data.toPandas()\n",
    "salesperson_data.to_csv('salesperson.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8af7150-61f9-482f-be8c-e5a05bac5753",
   "metadata": {},
   "source": [
    "#### Use PySpark to Read data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7d1df1-3ed1-463b-a1f2-b2946689c2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----------+------+-------------+------------+-------------+\n",
      "|SalesPersonID|TerritoryID|SalesQuota| Bonus|CommissionPct|    SalesYTD|SalesLastYear|\n",
      "+-------------+-----------+----------+------+-------------+------------+-------------+\n",
      "|          268|       NULL|      NULL|   0.0|          0.0| 677558.4653|          0.0|\n",
      "|          275|        2.0|  300000.0|4100.0|        0.012|4557045.0459| 1750406.4785|\n",
      "+-------------+-----------+----------+------+-------------+------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_salesperson = spark.read.option(\"header\", True).csv(\"salesperson.csv\")\n",
    "df_dim_salesperson.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb21c447-876a-4365-b314-62e8a52cf816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salesperson_key</th>\n",
       "      <th>salesperson_id</th>\n",
       "      <th>SalesQuota</th>\n",
       "      <th>SalesYTD</th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>SalesLastYear</th>\n",
       "      <th>Bonus</th>\n",
       "      <th>CommissionPct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>None</td>\n",
       "      <td>677558.4653</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>275</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>4557045.0459</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1750406.4785</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salesperson_key salesperson_id SalesQuota      SalesYTD TerritoryID  \\\n",
       "0                1            268       None   677558.4653        None   \n",
       "1                2            275   300000.0  4557045.0459         2.0   \n",
       "\n",
       "  SalesLastYear   Bonus CommissionPct  \n",
       "0           0.0     0.0           0.0  \n",
       "1  1750406.4785  4100.0         0.012  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'employee_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_salesperson = df_dim_salesperson.withColumnRenamed(\"SalesPersonID\", \"salesperson_id\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_salesperson.createOrReplaceTempView(\"salesperson\")\n",
    "sql_salesperson = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY salesperson_id) AS salesperson_key\n",
    "    FROM salesperson;\n",
    "\"\"\"\n",
    "df_dim_salesperson = spark.sql(sql_salesperson)\n",
    "\n",
    "ordered_columns = ['salesperson_key', 'salesperson_id', 'SalesQuota', 'SalesYTD'\n",
    "                   , 'TerritoryID', 'SalesLastYear', 'Bonus', 'CommissionPct']\n",
    "\n",
    "df_dim_salesperson = df_dim_salesperson[ordered_columns]\n",
    "\n",
    "df_dim_salesperson.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c699113-e8e5-4a23-8ca6-697704200def",
   "metadata": {},
   "source": [
    "#### Save Salesperson Dimension in Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc51aaed-76d8-4ac4-8765-142a02e66690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_salesperson.write.saveAsTable(f\"{dest_database}.dim_salesperson\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "448eb2fc-e977-4a8c-8781-cbbfdb08b7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|     salesperson_key|                 int|   NULL|\n",
      "|      salesperson_id|              string|   NULL|\n",
      "|          SalesQuota|              string|   NULL|\n",
      "|            SalesYTD|              string|   NULL|\n",
      "|         TerritoryID|              string|   NULL|\n",
      "|       SalesLastYear|              string|   NULL|\n",
      "|               Bonus|              string|   NULL|\n",
      "|       CommissionPct|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|northwind_data_la...|       |\n",
      "|               Table|     dim_salesperson|       |\n",
      "|        Created Time|Fri Apr 25 11:23:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.5|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/miamc...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salesperson_key</th>\n",
       "      <th>salesperson_id</th>\n",
       "      <th>SalesQuota</th>\n",
       "      <th>SalesYTD</th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>SalesLastYear</th>\n",
       "      <th>Bonus</th>\n",
       "      <th>CommissionPct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>268</td>\n",
       "      <td>None</td>\n",
       "      <td>677558.4653</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>275</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>4557045.0459</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1750406.4785</td>\n",
       "      <td>4100.0</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salesperson_key salesperson_id SalesQuota      SalesYTD TerritoryID  \\\n",
       "0                1            268       None   677558.4653        None   \n",
       "1                2            275   300000.0  4557045.0459         2.0   \n",
       "\n",
       "  SalesLastYear   Bonus CommissionPct  \n",
       "0           0.0     0.0           0.0  \n",
       "1  1750406.4785  4100.0         0.012  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_salesperson;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_salesperson LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0266c2b-0ce5-49bb-a958-815ade9e3f42",
   "metadata": {},
   "source": [
    "### Verify Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bce1c54-bdfb-4d6a-912f-3c7d961f9939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>northwind_data_lakehouse</td>\n",
       "      <td>dim_customers</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>northwind_data_lakehouse</td>\n",
       "      <td>dim_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>northwind_data_lakehouse</td>\n",
       "      <td>dim_products</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>northwind_data_lakehouse</td>\n",
       "      <td>dim_salesperson</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>salesperson</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  namespace        tableName  isTemporary\n",
       "0  northwind_data_lakehouse    dim_customers        False\n",
       "1  northwind_data_lakehouse         dim_date        False\n",
       "2  northwind_data_lakehouse     dim_products        False\n",
       "3  northwind_data_lakehouse  dim_salesperson        False\n",
       "4                                salesperson         True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25c3b-1f3f-4c82-a96f-6b8a97386bfe",
   "metadata": {},
   "source": [
    "### Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Orders</span> Fact Data  \n",
    "#### Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "268429b5-4a6b-4a48-81df-9f58a1bf8262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sales_1.json</td>\n",
       "      <td>114732</td>\n",
       "      <td>2025-04-24 16:14:31.247002840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sales_2.json</td>\n",
       "      <td>101409</td>\n",
       "      <td>2025-04-24 16:14:29.302740335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sales_3.json</td>\n",
       "      <td>112668</td>\n",
       "      <td>2025-04-24 16:14:00.144544125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name    size             modification_time\n",
       "0  sales_1.json  114732 2025-04-24 16:14:31.247002840\n",
       "1  sales_2.json  101409 2025-04-24 16:14:29.302740335\n",
       "2  sales_3.json  112668 2025-04-24 16:14:00.144544125"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(sales_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0505d-7f5a-4b71-9cf7-98716dbafc71",
   "metadata": {},
   "source": [
    "#### Create the Bronze Layer: Stage <span style=\"color:darkred\">Orders Fact table</span> Data\n",
    "##### Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e918a55d-4c87-422d-8227-2a6d8f8c65ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"schemaLocation\", sales_output_bronze) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(sales_stream_dir)\n",
    ")\n",
    "\n",
    "df_sales_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80013445-2c49-4606-8464-988deeafa27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_checkpoint_bronze = os.path.join(sales_output_bronze, '_checkpoint')\n",
    "\n",
    "sales_bronze_query = (\n",
    "    df_sales_bronze\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    \n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"sales_bronze\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", sales_checkpoint_bronze) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(sales_output_bronze)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "905225d3-47c9-47cd-a365-4ca94b699e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: c337c195-9439-4318-a1fd-de6f07aa78b4\n",
      "Query Name: sales_bronze\n",
      "Query Status: {'message': 'Writing offsets to log', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {sales_bronze_query.id}\")\n",
    "print(f\"Query Name: {sales_bronze_query.name}\")\n",
    "print(f\"Query Status: {sales_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8992464f-56b1-491c-b257-94946db558c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f149e91-eb3b-407d-89dc-5cf3c67e73cf",
   "metadata": {},
   "source": [
    "#### Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9474a08f-b64e-4de9-bc7c-93803b091686",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_order_date = df_dim_date.select(col(\"date_key\").alias(\"order_date_key\"), col(\"full_date\").alias(\"order_full_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8698f51-044d-4701-88b0-160b2e80f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_silver = spark.readStream.format(\"parquet\").load(sales_output_bronze) \\\n",
    "    .join(broadcast(df_dim_customers), \"customer_key\", \"inner\") \\\n",
    "    .join(broadcast(df_dim_salesperson), \"salesperson_key\", \"inner\") \\\n",
    "    .join(broadcast(df_dim_products), \"product_key\", \"inner\") \\\n",
    "    .join(broadcast(df_dim_order_date), df_dim_order_date.order_full_date.cast(DateType()) == col(\"order_full_date\").cast(DateType()), \"inner\") \\\n",
    "    .select(\n",
    "        col(\"SalesOrderID\").cast(LongType()),\n",
    "        df_dim_customers.customer_key.cast(LongType()),\n",
    "        df_dim_salesperson.salesperson_key.cast(LongType()),\n",
    "        df_dim_products.product_key.cast(LongType()),\n",
    "        df_dim_order_date.order_date_key.cast(LongType()),\n",
    "        col(\"UnitPrice\"),\n",
    "        col(\"OrderQty\"),\n",
    "        col(\"UnitPriceDiscount\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2332124b-e0de-4a26-9932-4f4a41bbe389",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sales_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0166dd23-0d54-43ab-b29d-21f5ef934a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SalesOrderID: long (nullable = true)\n",
      " |-- customer_key: long (nullable = true)\n",
      " |-- salesperson_key: long (nullable = false)\n",
      " |-- product_key: long (nullable = true)\n",
      " |-- order_date_key: long (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- OrderQty: long (nullable = true)\n",
      " |-- UnitPriceDiscount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb939e30-7aa9-49d9-b9ac-16234d86f726",
   "metadata": {},
   "source": [
    "##### Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6a5e202-b8bb-4122-8b78-6238bef16dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_checkpoint_silver = os.path.join(sales_output_silver, '_checkpoint')\n",
    "\n",
    "sales_silver_query = (\n",
    "    df_sales_silver.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"sales_silver\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", sales_checkpoint_silver) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(sales_output_silver)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7dc6547-b57d-414d-9000-85d27bbc8fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 3f7f4425-abfd-47f9-8f40-4cac4e61aaec\n",
      "Query Name: sales_silver\n",
      "Query Status: {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {sales_silver_query.id}\")\n",
    "print(f\"Query Name: {sales_silver_query.name}\")\n",
    "print(f\"Query Status: {sales_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ed6d0b4-52ac-43ef-bc7a-05d541117638",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sales_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012485b6-15c7-4df9-8df6-855fab3df433",
   "metadata": {},
   "source": [
    "#### Create Gold Layer: Perform Aggregations\n",
    "#### Sales per rep by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00546281-5420-457d-806c-3052bbf9517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales_by_rep_gold = spark.readStream.format(\"parquet\").load(sales_output_silver) \\\n",
    "    .join(df_dim_date, col(\"order_date_key\") == col(\"date_key\")) \\\n",
    "    .groupBy(\"salesperson_key\", \"month_of_year\", \"month_name\") \\\n",
    "    .agg(sum(\"OrderQty\").alias(\"units_sold\"),\n",
    "        sum((col(\"UnitPrice\") * col(\"OrderQty\") * (1 - col(\"UnitPriceDiscount\")))).alias(\"revenue\")\n",
    "    ) \\\n",
    "    .orderBy(\"salesperson_key\", \"month_of_year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8dd4797d-8307-4e0e-a487-605a10e7a7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- salesperson_key: long (nullable = true)\n",
      " |-- month_of_year: byte (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- units_sold: long (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_by_rep_gold.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f698a5f-0604-477e-853d-99a8d50cdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_gold_query = (\n",
    "    df_sales_by_rep_gold.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"sales_by_rep\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5917596-9256-4bf3-878e-ca01dd6f06de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 1 batchs\n"
     ]
    }
   ],
   "source": [
    "wait_until_stream_is_ready(sales_gold_query, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b8f8077a-b1f1-4eea-8702-9a7cc189ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- salesperson_key: long (nullable = true)\n",
      " |-- month_of_year: byte (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- units_sold: long (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_sales_per_rep = spark.sql(\"SELECT * FROM sales_by_rep\")\n",
    "df_fact_sales_per_rep.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cebae5e-14c1-4e22-a2d7-62dff749b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_sales_per_rep_gold_final = df_fact_sales_per_rep \\\n",
    ".select(col(\"salesperson_key\").alias(\"Salesperson Key\"), \\\n",
    "        col(\"revenue\").alias(\"Salesperson Revenue\"), \\\n",
    "        col(\"units_sold\").alias(\"Total Units Sold\")) \\\n",
    ".orderBy(asc(\"month_of_year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e53371af-867e-471b-86b5-9da7219c5b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Salesperson Key</th>\n",
       "      <th>Salesperson Revenue</th>\n",
       "      <th>Total Units Sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.005743e+06</td>\n",
       "      <td>6138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5.150264e+07</td>\n",
       "      <td>76725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5.113608e+07</td>\n",
       "      <td>68541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7.384120e+07</td>\n",
       "      <td>96844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3.910760e+07</td>\n",
       "      <td>45694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>6</td>\n",
       "      <td>9.874770e+07</td>\n",
       "      <td>119009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>7</td>\n",
       "      <td>3.510461e+07</td>\n",
       "      <td>35464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>8</td>\n",
       "      <td>2.920725e+07</td>\n",
       "      <td>76725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>9</td>\n",
       "      <td>9.094743e+07</td>\n",
       "      <td>152427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>10</td>\n",
       "      <td>2.992034e+07</td>\n",
       "      <td>30690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Salesperson Key  Salesperson Revenue  Total Units Sold\n",
       "0                  1         7.005743e+06              6138\n",
       "1                  2         5.150264e+07             76725\n",
       "2                  3         5.113608e+07             68541\n",
       "3                  4         7.384120e+07             96844\n",
       "4                  5         3.910760e+07             45694\n",
       "..               ...                  ...               ...\n",
       "115                6         9.874770e+07            119009\n",
       "116                7         3.510461e+07             35464\n",
       "117                8         2.920725e+07             76725\n",
       "118                9         9.094743e+07            152427\n",
       "119               10         2.992034e+07             30690\n",
       "\n",
       "[120 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_sales_per_rep_gold_final.write.saveAsTable(f\"{dest_database}.sales_by_rep\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.sales_by_rep\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
